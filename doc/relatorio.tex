\documentclass[a4paper, 12pt]{article}

\usepackage[portuguese]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{url}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{tikz}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{algorithm}
\usepackage{algpseudocode}

\graphicspath{{../img/}}

\newcommand{\rom}[1]{\uppercase\expandafter{\romannumeral #1\relax}}

\linespread{1.25}

\begin{document}

\begin{titlepage}
    \centering
    \vspace*{4cm}
    \textbf{\huge{Gradiente Descendente Estocástico -- MAC5832}}\\

    \vskip 1cm

    Pedro Henrique Rocha Bruel

    \emph{phrb@ime.usp.br}

    \emph{nUSP: 7143336}

    \vfill
    \normalsize{\emph{DCC - IME\\
    Universidade de São Paulo}\\}
    \normalsize{São Paulo, \today}
\end{titlepage}

\section{Introdução} \label{sec:intro}

Este documento descreve as implementações realizadas para o Trabalho $2$ da
disciplina \textit{MAC5832}, apresenta análises de desempenho desses algoritmos
segundo seus parâmetros, e compara o desempenho dessas implementações com
algoritmos do pacote \texttt{Scikit Learn}.

Foram implementados um algoritmo para Gradiente Descendente Estocástico, dois
modelos de regressão e um método de regularização, totalizando quatro
combinações de modelos: Regressão Linear, Regressão Linear com Regularização
$L_2$, Regressão Logística e Regressão Logística com Regularização $L_2$.

\subsection{Estrutura de Diretórios}

Os diretórios deste trabalho estão organizados da
seguinte forma:

\paragraph{\texttt{./doc}} Contém os arquivos necessários
para gerar este documento.

\paragraph{\texttt{./data}} Contém os conjuntos de dados
disponibilizados para treinamento e testes.

\paragraph{\texttt{./src}} Contém o código implementado
para o \textit{Perceptron} e \textit{Gradient Descent}.

\paragraph{\texttt{./src/plot}} Contém o código para
gerar as figuras com as visualizações, e também as figuras.

\section{Gradiente Estocástico Descendente}

O método do \textit{Gradiente Estocástico Descendente} é uma técnica iterativa
para minimização de funções duplamente diferenciáveis. No nosso caso, o método
é aplicado para minimizar o erro de classificação $E_{in}(\textbf{w},
\textbf{X}, \textbf{y})$, onde $\textbf{w}$ é um vetor de pesos, \textbf{X} é
uma matriz de exemplos, ou amostras, e \textbf{y} é um vetor de classificações,
onde cada classificação $\textbf{y}(i)$ corresponde a um exemplo
$\textbf{X}(i)$.

Para encontrar o vetor de pesos $\textbf{w}^{*}$ que minimiza $E_{in}(\cdot)$,
cada iteração do método do Gradiente Estocástico Descendente calcula o
\textit{gradiente} $\nabla{}E_{in}(\cdot)$ e atualiza o vetor atual de pesos.

A técnica do Gradiente Estocástico Descendente pode ser compreendida
intuitivamente como a \lq\lq{}exploração\rq\rq{} de um terreno com diferentes
inclinações e profundidades, utilizando uma bola sujeita à força da gravidade.
A bola tenderá a encontrar o ponto do terreno com menor profundidade, mas o
local inicial terá bastante impacto no local final. Formas de atenuar esse
problema incluem utilizar várias bolas ao mesmo tempo, ou bolas com
\lq\lq{}coeficientes de atrito\rq\rq{} diferentes.

\subsection{Algoritmo}

A implementação da técnica do Gradiente Estocástico Descendente deste trabalho
foi feita na linguagem \texttt{Python 3.5.1} e está descrita em pseudocódigo no
Algoritmo \ref{alg:GSD}. O resto dessa Seção descreve a implementação em
detalhes.

\subsubsection{Parâmetros}

A implementação foi feita de forma \textit{modular}, permitindo que funções
para o cálculo do gradiente e da taxa de aprendizado fossem passadas como
parâmetro. Esta Seção descreve os parâmetros para o algoritmo implementado.

\paragraph{Taxa de Aprendizado inicial ($\alpha^{\prime}$):}

Em todos os experimentos deste trabalho utilizei a seguinte função
para o cálculo da taxa de aprendizado $\alpha(i)$, onde $i$ é o número da
iteração atual do algoritmo, e $\alpha^{\prime}$ é a taxa de aprendizado
inicial:

\begin{align*}
    \alpha(i) = \frac{\alpha^{\prime}}{1 + log_{2}i}
\end{align*}

Seguindo a metáfora para a compreensão intuitiva da técnica, a ideia é
modificar o \lq\lq{}coeficiente de atrito\rq\rq{}, isto é, a taxa de
aprendizado, para que em iterações inicias sejam permitidas variações maiores
no vetor de pesos, e posteriomente na execução do algoritmo, apenas
\lq\lq{}saltos\rq\rq{} menores.

\paragraph{Função do Gradiente ($\nabla{}E_{in}(\cdot)$):}

A expressão para o cálculo do gradiente será diferente, de acordo com o modelo
de regressão que utilizarmos em conjunto com a técnica de Gradiente Estocástico
Descendente. As expressões para cálculo do gradiente dos quatros modelos de
regressão implementados neste trabalho estão descritas na Seção \ref{sec:regr}.

A implementação de todas as funções de gradiente recebe como entrada um vetor
de pesos \textbf{w}, e um subconjunto dos exemplos em \textbf{X}, junto com
suas classificações em \textbf{y} correspondentes. A saída dessas funções é o
gradiente calculado de acordo com o modelo de regressão correspondente.

\paragraph{Número de Iterações ($I$):}

É o número de iterações realizadas antes de se devolver o vetor final de pesos
$\textbf{w}(I)$.

\paragraph{Vetor Inicial de Pesos ($\textbf{w}(0)$):}

Deve ter o mesmo número de elementos em cada exemplo de \textbf{X}.
É inicializado com zeros.

\paragraph{Matriz de Exemplos ($\textbf{X}$):}

Contém os exemplos para teste em suas linhas.

\paragraph{Vetor de Classificações ($\textbf{y}$):}

A posição $\textbf{y}(i)$ contém a classificação
do exemplo $\textbf{X}(i)$.

\paragraph{Tamanho do \textit{Batch} ($\sigma$):}

Determina o tamanho do subconjunto de exemplos que será utilizado em cada
iteração. O tamanho do conjunto de dados não permitiu que os gradientes
fossem calculados usando todo o conjunto de uma só vez. Para contornar
isso utilizei um parâmetro que determina a \textit{porcentagem} do
conjunto que será utilizada nas iterações.

A cada iteração são calculados os conjuntos $\textbf{X}[\sigma]$ e
$\textbf{y}[\sigma]$, que contém $m$  elementos escolhidos \textit{ao acaso},
onde $m = \left\vert{X}\right\vert \cdot \sigma$.  Dessa forma é possível usar
todo o conjunto de dados sem pagar o custo dos produtos escalares de matrizes
enormes.

\subsubsection{Saída}

\paragraph{Vetor Final de Pesos ($\textbf{w}(I)$):}

Contém um peso para cada característica dos exemplos de teste
em \textbf{X}. É usado posteriormente para fazer as predições
de classificação.

\begin{algorithm}[htpb]
    \caption{Gradiente Estocástico Descendente \\ com Taxa de Aprendizado Variante}
    \label{alg:GSD}
    \begin{algorithmic}[1]
        \Statex \item[\textbf{Input:}]
        \Statex $\nabla{}E_{in}(\cdot)$ \Comment Função para cálculo do Gradiente
        \Statex $I$ \Comment Número de Iterações
        \Statex $\sigma$ \Comment Tamanho do \textit{batch}
        \Statex $\alpha^{\prime}$ \Comment Taxa de Aprendizado inicial
        \Statex $\textbf{w}(0)$ \Comment Vetor inicial de pesos
        \Statex $\textbf{X}$ \Comment Matriz de exemplos
        \Statex $\textbf{y}$ \Comment Vetor de classificações
        \Statex \item[\textbf{Output:}]
        \Statex $\textbf{w}(I)$ \Comment Vetor final de pesos
        \Statex
        \For{$i = 1, 2, \dots, I$}
        \State Calcule $\textbf{g}_i = \nabla{}E_{in}(\textbf{w}(i),\: \textbf{X}[\sigma],\: \textbf{y}[\sigma])$
        \State Calcule $\alpha(i) = \frac{\alpha^{\prime}}{1 + log_{2}i}$
        \State Calcule $\textbf{w}(i) = \textbf{w}(i -1) - \alpha{}\textbf{g}_i$
        \EndFor
        \State \Return $\textbf{w}(I)$
    \end{algorithmic}
\end{algorithm}


\section{Modelos de Regressão} \label{sec:regr}

Esta Seção discute os modelos de regressão
e suas implementações, feitas em \texttt{Python 3.5.1} e usando o
módulo \texttt{numpy}.

\subsection{Regressão Linear}

\subsection{Regressão Linear com Regularização $L_2$}

\subsection{Regressão Logística}

\subsection{Regressão Logística com Regularização $L_2$}

\section{Resultados}

\begin{figure}[htpb]
    \centering
    \begin{subfigure}[htpb]{0.45\textwidth}
        \includegraphics[width=\textwidth]{acc_vs_iterations_linreg}
        \caption{}
        \label{fig:it_linreg}
    \end{subfigure}
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[htpb]{0.45\textwidth}
        \includegraphics[width=\textwidth]{acc_vs_iterations_linregL2}
        \caption{}
        \label{fig:it_linregL2}
    \end{subfigure}
    \hfill %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[htpb]{0.45\textwidth}
        \includegraphics[width=\textwidth]{acc_vs_iterations_logreg}
        \caption{}
        \label{fig:it_logreg}
    \end{subfigure}
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[htpb]{0.45\textwidth}
        \includegraphics[width=\textwidth]{acc_vs_iterations_logregL2}
        \caption{}
        \label{fig:it_logregL2}
    \end{subfigure}
    \caption{Acurácia vs. Número de Iterações ($I$) do Gradiente Estocástico
    Descendente, com parâmetros fixos: $\lambda=0.0051$, $\alpha=0.5$,
    $\sigma=10^{-3}$}\label{fig:it}
\end{figure}

\begin{figure}[htpb]
    \centering
    \begin{subfigure}[htpb]{0.45\textwidth}
        \includegraphics[width=\textwidth]{acc_vs_rate_linreg}
        \caption{}
        \label{fig:rate_lingreg}
    \end{subfigure}
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[htpb]{0.45\textwidth}
        \includegraphics[width=\textwidth]{acc_vs_rate_linregL2}
        \caption{}
        \label{fig:rate_linregL2}
    \end{subfigure}
    \hfill %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[htpb]{0.45\textwidth}
        \includegraphics[width=\textwidth]{acc_vs_rate_logreg}
        \caption{}
        \label{fig:rate_logreg}
    \end{subfigure}
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[htpb]{0.45\textwidth}
        \includegraphics[width=\textwidth]{acc_vs_rate_logregL2}
        \caption{}
        \label{fig:rate_logregL2}
    \end{subfigure}
    \caption{Acurácia vs. Taxa de Aprendizado ($\alpha$), com parâmetros
    fixos: $\lambda=0.0051$, $I=15000$, $\sigma=10^{-3}$}\label{fig:rate}
\end{figure}

\begin{figure}[htpb]
    \centering
    \begin{subfigure}[htpb]{0.45\textwidth}
        \includegraphics[width=\textwidth]{acc_vs_batchp_linreg}
        \caption{}
        \label{fig:batch_linreg}
    \end{subfigure}
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[htpb]{0.45\textwidth}
        \includegraphics[width=\textwidth]{acc_vs_batchp_linregL2}
        \caption{}
        \label{fig:batch_linregL2}
    \end{subfigure}
    \hfill %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[htpb]{0.45\textwidth}
        \includegraphics[width=\textwidth]{acc_vs_batchp_logreg}
        \caption{}
        \label{fig:batch_logreg}
    \end{subfigure}
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[htpb]{0.45\textwidth}
        \includegraphics[width=\textwidth]{acc_vs_batchp_logregL2}
        \caption{}
        \label{fig:batch_logregL2}
    \end{subfigure}
    \caption{Acurácia vs. Tamanho do \textit{Batch} ($\sigma$), com
    parâmetros fixos: $\lambda=0.0051$, $I=500$, $\alpha=0.5$}\label{fig:batch}
\end{figure}

\begin{figure}[htpb]
    \centering
    \begin{subfigure}[htpb]{0.45\textwidth}
        \includegraphics[width=\textwidth]{acc_vs_lambda_linregL2}
        \caption{}
        \label{fig:lambda_linregL2}
    \end{subfigure}
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[htpb]{0.45\textwidth}
        \includegraphics[width=\textwidth]{acc_vs_lambda_logregL2}
        \caption{}
        \label{fig:lambda_logregL2}
    \end{subfigure}
    \caption{Acurácia vs. Parâmetro $\lambda$, com parâmetros fixos:
    $\sigma=0.003$, $\alpha=2.0$, $I=500$}\label{fig:lambda}
\end{figure}

\begin{figure}[htpb]
    \centering
    \begin{subfigure}[htpb]{0.45\textwidth}
        \includegraphics[width=\textwidth]{acc_vs_batchp_skl_linreg}
        \caption{}
        \label{fig:batch_skl_linreg}
    \end{subfigure}
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[htpb]{0.45\textwidth}
        \includegraphics[width=\textwidth]{acc_vs_batchp_skl_linregL2}
        \caption{}
        \label{fig:batch_skl_linregL2}
    \end{subfigure}
    \hfill %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[htpb]{0.45\textwidth}
        \includegraphics[width=\textwidth]{acc_vs_batchp_skl_logregL2}
        \caption{}
        \label{fig:batch_skl_logregL2}
    \end{subfigure}
    \caption{Acurácia vs. Tamanho do \textit{Batch} ($\sigma$)
    para algoritmos do \texttt{Scikit Learn}}\label{fig:batch_skl}
\end{figure}

\begin{table}[htpb]
    \centering
    \begin{tabular}{@{}lcc@{}}
        \textbf{Modelos} & \multicolumn{2}{c}{\textbf{Parâmetros}} \\ \toprule
        & $\lambda$ & $\alpha$ \\ \midrule
        Regr. Lin. & - & $0.25$ \\
        Regr. Lin. ($L_2$) & $2^{-4}$ & $0.25 $       \\
        Regr. Log. & - & $2$ \\
        Regr. Log. ($L_2$) & $2^{-10}$ & $0.5$ \\ \bottomrule
    \end{tabular}
    \caption{Escolha de parâmetros para as implementações, após os
    experimentos. Todos os modelos usaram o mesmo número de iterações,
    $I = 10^{4}$, e tamanho de \textit{batch}, $\sigma = 2^{-6}$ (aprox.
    $1.5\%$ do conjunto de dados por iteração)}
    \label{tab:par}
\end{table}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=.8\textwidth]{acc_vs_algorithm}
    \caption{Acurácia dos algoritmos implementados e do
    \texttt{Scikit Learn}. Cada algoritmo foi executado
    com diferentes parâmetros otimizados}
    \label{fig:algorithm}
\end{figure}

\section{Conclusão} \label{sec:concl}

A minha implementação do \textit{Perceptron} teve um desempenho melhor do que a
implementação do \textit{Gradient Descent} mas, usando metade dos exemplos como
treinamento e outra metade como teste, ambos os algoritmos atingiram resultados
de por volta de $10\%$ de exemplos classificados erroneamente.

\end{document}
